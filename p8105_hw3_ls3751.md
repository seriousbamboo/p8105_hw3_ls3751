p8105\_hw3\_ls3751
================
Liucheng Shi

*packages required*

``` r
library(tidyverse)
library(p8105.datasets)
library(ggridges)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### Problem 1

``` r
data("instacart")
```

This dataset contains 1384617 rows and … columns.

Observations are the level of items in orders by user. There are user /
order variables – user ID, order ID, order day, and order hour. There
are also item variables – name, aisle, department, and some numeric
codes.

How many aisles, and which are most items from?

``` r
instacart %>% 
    count(aisle) %>% 
    arrange(desc(n))
```

    ## # A tibble: 134 x 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # ... with 124 more rows

Let’s make a plot

``` r
instacart %>% 
    count(aisle) %>% 
    filter(n > 10000) %>% 
    mutate(
        aisle = factor(aisle),
        aisle = fct_reorder(aisle, n)
    ) %>% 
    ggplot(aes(x = aisle, y = n)) + 
    geom_point() + 
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

<img src="p8105_hw3_ls3751_files/figure-gfm/unnamed-chunk-3-1.png" width="90%" />

Let’s make a table\!\!

``` r
instacart %>% 
    filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
    group_by(aisle) %>% 
    count(product_name) %>% 
    mutate(rank = min_rank(desc(n))) %>% 
    filter(rank < 4) %>% 
    arrange(aisle, rank) %>% 
    knitr::kable()
```

| aisle                      | product\_name                                 |    n | rank |
| :------------------------- | :-------------------------------------------- | ---: | ---: |
| baking ingredients         | Light Brown Sugar                             |  499 |    1 |
| baking ingredients         | Pure Baking Soda                              |  387 |    2 |
| baking ingredients         | Cane Sugar                                    |  336 |    3 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |    1 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |    2 |
| dog food care              | Small Dog Biscuits                            |   26 |    3 |
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |    1 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |    2 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |    3 |

Apples vs ice cream..

``` r
instacart %>% 
    filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
    group_by(product_name, order_dow) %>% 
    summarize(mean_hour = mean(order_hour_of_day)) %>% 
    pivot_wider(
        names_from = order_dow,
        values_from = mean_hour
    )
```

    ## `summarise()` regrouping output by 'product_name' (override with `.groups` argument)

    ## # A tibble: 2 x 8
    ## # Groups:   product_name [2]
    ##   product_name       `0`   `1`   `2`   `3`   `4`   `5`   `6`
    ##   <chr>            <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
    ## 1 Coffee Ice Cream  13.8  14.3  15.4  15.3  15.2  12.3  13.8
    ## 2 Pink Lady Apples  13.4  11.4  11.7  14.2  11.6  12.8  11.9

### Problem 2

#### 1.1 load the dataset and tidy data

``` r
accel_df = 
  read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440, 
    names_to = "minute",
    names_prefix = "activity_", 
    values_to = "counts"
  )
```

#### 1.2 Create a variable that indicate the weekday/weekend status and modify variable formats

``` r
accel_df = accel_df %>% 
  mutate(
    weekday_weekend = case_when(
      day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday") ~ "weekday",
      day %in% c("Saturday", "Sunday") ~ "weekend",
      TRUE ~ "")
      ) %>% 
  mutate(
    minute = as.numeric(minute),
    week = factor(week),
    day = forcats::fct_relevel(day, c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")
    )) %>% 
  relocate(weekday_weekend, .before = minute)
```

#### 1.3 Overview

After data cleaning and wrangling, this dataset has 50400 observations
with 6 variables, including week, day\_id, day, weekday\_weekend,
minute, counts.

  - *week\_id*: the week indicator ranging from 1 to 5.
  - *day\_id*: the day indicator ranging from 1 to 35 (5 weeks in
    total).
  - *day* & *weekday\_weekend*: ex: Sunday, weekend.
  - *minutes*: the recorded time (1440 min a day).
  - *counts*: the count of activities measured by accelerometer.

#### 2\. Traditional analyses of accelerometer

``` r
accel_df %>% 
  group_by(week, day) %>% 
  summarise(activity_sum = sum(counts, na.rm = T)) %>% 
  pivot_wider(
   names_from = week,
   names_prefix = "week",
   values_from = activity_sum
  ) %>% 
  knitr::kable(digits = 1)
```

    ## `summarise()` regrouping output by 'week' (override with `.groups` argument)

| day       |    week1 |  week2 |  week3 |  week4 |  week5 |
| :-------- | -------: | -----: | -----: | -----: | -----: |
| Sunday    | 631105.0 | 422018 | 467052 | 260617 | 138421 |
| Monday    |  78828.1 | 295431 | 685910 | 409450 | 389080 |
| Tuesday   | 307094.2 | 423245 | 381507 | 319568 | 367824 |
| Wednesday | 340115.0 | 440962 | 468869 | 434460 | 445366 |
| Thursday  | 355923.6 | 474048 | 371230 | 340291 | 549658 |
| Friday    | 480542.6 | 568839 | 467420 | 154049 | 620860 |
| Saturday  | 376254.0 | 607175 | 382928 |   1440 |   1440 |

Besides the extreme large activity counts on Sunday Week 1 and two
extreme small counts on Saturdays Week 4 and 5, this table is not easy
to interpret and there is no apparent trend detected.

Ranking with the sum of activity counts.

``` r
accel_df %>% 
  group_by(week, day) %>% 
  summarise(activity_sum = sum(counts, na.rm = T)) %>% 
  mutate(rank = min_rank(desc(activity_sum))) %>% 
  select(-activity_sum) %>% 
  pivot_wider(
   names_from = week,
   names_prefix = "week",
   values_from = rank
  ) %>% 
  knitr::kable()
```

    ## `summarise()` regrouping output by 'week' (override with `.groups` argument)

| day       | week1 | week2 | week3 | week4 | week5 |
| :-------- | ----: | ----: | ----: | ----: | ----: |
| Sunday    |     1 |     6 |     4 |     5 |     6 |
| Monday    |     7 |     7 |     1 |     2 |     4 |
| Tuesday   |     6 |     5 |     6 |     4 |     5 |
| Wednesday |     5 |     4 |     2 |     1 |     3 |
| Thursday  |     4 |     3 |     7 |     3 |     2 |
| Friday    |     2 |     2 |     3 |     6 |     1 |
| Saturday  |     3 |     1 |     5 |     7 |     7 |

The table using ranks within the week is not easy to read neither.

#### 3\. Make a single-panel plot that shows the 24-hour activity time courses for each day

``` r
accel_df %>% 
  group_by(week,day) %>%
  mutate(hour = floor((minute-1)/60)) %>%
  ungroup() %>% 
  group_by(week,day,hour) %>% 
  summarise(activity_mean = mean(counts, na.rm = T)) %>% 
  ggplot(aes(x = hour, y = activity_mean)) +
  geom_point(aes(color = day), alpha = .5) +
  geom_smooth()
```

    ## `summarise()` regrouping output by 'week', 'day' (override with `.groups` argument)

    ## `geom_smooth()` using method = 'loess' and formula 'y ~ x'

<img src="p8105_hw3_ls3751_files/figure-gfm/unnamed-chunk-10-1.png" width="90%" />

### Problem 3

#### 1\. Overview

``` r
library(lubridate)
data("ny_noaa")
range(pull(ny_noaa,date))
## [1] "1981-01-01" "2010-12-31"
```

This dataset has 2595176 observations collected from 747 weather
stations with 7 variables.

  - *id*: the indicator of which site did the record.
  - *date*: the recorded dat from 1981.1.1 to 2010.12.31.
  - *prcp*: the precipitation in mm.
  - *snow*: snowfall in mm.
  - *snwd*: snow depth in mm.
  - *tmax* and *tmin*: the maximum and minimum temperature recorded.

<!-- end list -->

``` r
ny_noaa %>% 
  summarise(tmax_na = (sum(is.na(tmax)))/nrow(ny_noaa),
            tmin_na = (sum(is.na(tmin)))/nrow(ny_noaa),
            prcp_na = (sum(is.na(prcp)))/nrow(ny_noaa),
            snow_na = (sum(is.na(snow)))/nrow(ny_noaa),
            snwd_na = (sum(is.na(snwd)))/nrow(ny_noaa))
```

    ## # A tibble: 1 x 5
    ##   tmax_na tmin_na prcp_na snow_na snwd_na
    ##     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>
    ## 1   0.437   0.437  0.0562   0.147   0.228

The missing values on tmin and tmax seems to be problematic (over 40%)
compared with other variables (less than 6%).

#### 2\. Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units.

``` r
ny_df <- ny_noaa %>% 
  mutate(year = year(date),
         month = month(date), 
         day = day(date)) %>%
  mutate(tmax = as.numeric(tmax)/10,
         tmin = as.numeric(tmin)/10,
         prcp = as.numeric(prcp)/10)
ny_df %>% 
  count(snow) %>% 
  arrange(desc(n))
## # A tibble: 282 x 2
##     snow       n
##    <int>   <int>
##  1     0 2008508
##  2    NA  381221
##  3    25   31022
##  4    13   23095
##  5    51   18274
##  6    76   10173
##  7     8    9962
##  8     5    9748
##  9    38    9197
## 10     3    8790
## # ... with 272 more rows
```

The most common observed value for snowfall is 0 since we do not expect
to have any snowfall during a long period of every year.

#### 3\. Make a two-panel plot showing the average max temperature in January and in July in each station across years.

``` r
ny_jan_jul = ny_df %>% 
  filter(month %in% c(1, 7)) %>% 
  group_by(id, year, month) %>% 
  summarize(avg_tmax = mean(tmax, na.rm = TRUE)) %>% 
  drop_na()
 
ny_jan_jul %>% 
  ggplot(aes(x = year, y = avg_tmax, color = id)) +
  geom_point(alpha = .5) +
  geom_path(alpha = .3) +
  labs(
    title = "Average max temperature in January and July",
    subtitle = "1981-2010, New York state",
    x = "Time (year)",
    y = "Average max temperature (C)",
    caption = "From NOAA") +
  scale_x_continuous(breaks = c(seq(1980,2010, by = 5)),
                   labels = c(seq(1980,2010, by = 5))) +
  facet_grid(.~month) +
  theme(legend.position = "none", axis.text.x = element_text(angle = 270,  vjust = 0.5,  hjust = 1))
```

<img src="p8105_hw3_ls3751_files/figure-gfm/unnamed-chunk-14-1.png" width="90%" />

The average max temperature in January distributed from -10 C to 10 C
with few outliers(e.g.1982, 1994, 2005). It seems to be centered at 0
C.  
The average max temperature in July has a smaller variation compared
with January, ranging from 20 C to 35 C. It seems to be centered at 27.5
C. A few more outliers were detected (1984, 1988, 2004, 2007).

#### 4\. Make a two-panel plot

##### 4.1 showing tmax vs tmin for the full dataset

``` r
tmax_tmin = ny_df %>% 
  drop_na(tmax,tmin) %>% 
  ggplot(aes(x = tmax, y = tmin)) +
  geom_hex() +
  labs(
    title = "Maximum temp vs. minimum ",
    subtitle = "1981-2010 New York state",
    x = "Max temperature (C)",
    y = "Min temperature (C)",
    caption = "From NOAA") +
  theme(legend.text = element_text(size = 5))
tmax_tmin
```

<img src="p8105_hw3_ls3751_files/figure-gfm/unnamed-chunk-15-1.png" width="90%" />

##### 4.2 make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year

``` r
snow_plot = ny_df %>%
  select(year, snow) %>% 
  filter(snow > 0 & snow < 100) %>% 
  mutate(year = factor(year)) %>% 
  ggplot(aes(x = snow, y = year)) + 
  geom_density_ridges(color = "red", fill = "green") + 
  labs(
    title = "Distribution of snowfall by year",
    subtitle = "1981-2010 New York state",
    x = "Snowfall (mm)",
    y = "Year")
snow_plot
```

    ## Picking joint bandwidth of 3.76

<img src="p8105_hw3_ls3751_files/figure-gfm/unnamed-chunk-16-1.png" width="90%" />

##### 4.3 show two plots using patchwork

``` r
tmax_tmin + snow_plot
```

<img src="p8105_hw3_ls3751_files/figure-gfm/unnamed-chunk-17-1.png" width="90%" />
